{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"pyERGM - A Python implementation for ERGM's","text":"<p>An exponential random graphs model (ERGM) is a statistical model that describes a distribution of random graphs. This package provides a simple and easy way to fit and sample from ERGMs.</p> <p>An ERGM defines a random variable \\(\\mathbf{Y}\\), which is simply a random graph on \\(n\\) nodes. The probability of observing a graph \\(y\\in\\mathcal{Y}\\) is given by -</p> \\[\\Pr(\\mathbf{Y}=y | \\theta) = \\frac{\\exp(\\theta^Tg(y))}{\\sum_{z\\in\\mathcal{Y}} \\exp(\\theta^Tg(z))}\\] <p>where \\(\\mathcal{Y}\\) is the set of all \\(n\\) node graphs, \\(g(y)\\) is a vector of statistics that describe the graph \\(y\\), and \\(\\theta \\in \\mathbb{R}^q\\) is a vector of model parameters. Each graph is represented by a binary adjacency matrix, where \\(y_{ij}=1\\) if there is an edge between nodes \\(i\\) and \\(j\\) (and \\(y_{ji}=1\\) in the undirected case).</p> <p>Fitting a model for even moderately large graphs can be a computationally challenging task. pyERGM keeps this in mind and is implemented to be efficient and scalable by using <code>numpy</code> and <code>Numba</code>, as well as providing an interface for fitting models on a distributed computing environment.</p>"},{"location":"#installation","title":"Installation","text":"<p>TODO</p>"},{"location":"#getting-started","title":"Getting started","text":"<p>Fitting an ERGM model requires a graph and a set of statistics that describe the graph. The model is then fit by maximizing the likelihood of the observed graph under the model. </p> <p>The following example demonstrates how to fit a simple ERGM model from Sampson's monastery data.</p> <pre><code>from pyERGM.ergm import ERGM\nfrom pyERGM.metrics import *\nfrom pyERGM.datasets import load_sampson\n\nsampson_matrix = load_sampson()\n\nnum_nodes = sampson_matrix.shape[0]\nis_directed = True\nmetrics = [NumberOfEdgesDirected(), TotalReciprocity()]\n\nmodel = ERGM(num_nodes, metrics, is_directed=is_directed)\nmodel.fit(sampson_matrix)\n</code></pre> <p>The above example fits a model from the Sampson's monastery data using the number of edges and total reciprocity as statistics. The graph is represented as an adjacency matrix, but pyERGM also supports graphs represented as <code>networkx</code> objects.</p>"},{"location":"custom_metrics/","title":"Creating custom metrics","text":"<p>pyERGM offers a simple and straightforward way to create custom metrics. </p> <p>Generally speaking, any sort of statistic that can be calculated from a graph can be implemented as a metric. All you need to do is - </p> <ol> <li>Create a new class that inherits from the <code>Metric</code> class</li> <li>Implement a <code>calculate</code> function that receives a graph and returns the calculated statistics.</li> </ol>"},{"location":"custom_metrics/#a-first-example","title":"A first example","text":"<p>Let's begin with a very simple metric, that counts the number of nodes in a directed graph, with in-degree \\(d \\geq \\frac{n}{2}\\) - </p> <pre><code>from pyERGM.metrics import Metric\n\nclass NumberOfBigIndegreeNodes(Metric):\n    def __init__(self):\n        super().__init__(requires_graph=False)\n        self._is_directed = True\n        self._is_dyadic_independent = True\n\n    def calculate(self, input_graph: np.ndarray):\n        n = input_graph.shape[0]\n\n        indeg = np.sum(input_graph, axis=0)\n        big_indegree_nodes = np.sum(indeg &gt;= n/2)\n\n        return big_indegree_nodes\n</code></pre> <p>which would then be used as follows - </p> <pre><code>num_big_indegree_nodes = NumberOfBigIndegreeNodes()\n\nW = np.array([\n    [0, 1, 0, 0],\n    [1, 0, 0, 1],\n    [1, 0, 0, 0],\n    [1, 0, 1, 0]\n])\nnum_of_big_indegree_nodes = num_big_indegree_nodes.calculate(W)\nprint(f\"# of nodes with in-degree &gt;= n/2: {num_of_big_indegree_nodes}\")\n</code></pre> <p>Output -  <pre><code># of nodes with in-degree &gt;= n/2: 1\n</code></pre></p> <p> Let's break down the steps in the above example -</p> <ol> <li>We began by creating a new class <code>NumberOfBigIndegreeNodes</code> that inherits from the <code>Metric</code> class.</li> <li>The <code>__init__()</code> function deals with 3 important attributes of the metric - <ul> <li><code>requires_graph=False</code> is passed to the parent class, and indicates that the metric does not require a <code>networkx</code> input graph to be passed to the <code>calculate</code> function. This implies that it expects a numpy adjacency matrix as input. (this can also be observed by the type hint we wrote in <code>calculate</code>)</li> <li><code>self._is_directed = True</code> indicates that the metric is designed to work with directed graphs.</li> <li><code>self._is_dyadic_independent = True</code> indicates that the metric is dyadic independent, meaning that the existence of an edge <code>i -&gt; j</code> does not depend on the presence of any other edge in the graph.  The fact that this is a dyadic independent metric means that <code>MPLE</code> can be performed for ERGM's using this metric.</li> </ul> </li> <li>The <code>calculate</code> function receives the input graph as a numpy adjacency matrix and calculates the number of nodes with in-degree greater than or equal to \\(\\frac{n}{2}\\) and returns a single scalar value.</li> </ol>"},{"location":"custom_metrics/#supporting-multiple-statistics","title":"Supporting multiple statistics","text":"<p>Some metrics return multiple statistics. For example, the <code>InDegree</code> metric calculates the indegree of each node in the graph. To support multiple statistics, the <code>calculate</code> function should simply return a collection of statistics, instead of a scalar value. This can either be a list or a numpy array. </p> <p>Moreover, metrics with multiple statistics should also support the <code>indices_to_ignore</code> attribute, which allows pyERGM to exclude certain statistics from the calculation (see the Introduction to graph metrics page for more information). Your new metric should allow the user to pass a list of indices to ignore to the metric constructor, and it has to save it as <code>self._indices_to_ignore</code> - </p> <pre><code>from copy import deepcopy\n\nclass MetricWithMultipleStatistics(Metric):\n    def __init__(self, indices_to_ignore=None):\n        super().__init__(requires_graph=False)\n        self._is_directed = True\n        self._is_dyadic_independent = True\n\n        if indices_to_ignore is None:\n            self._indices_to_ignore = []\n        else:\n            self._indices_to_ignore = deepcopy(indices_to_ignore)\n</code></pre> <p>Finally, the <code>calculate</code> method can use the <code>_indices_to_ignore</code> attribute to exclude certain statistics from the calculation - </p> <pre><code>def calculate(self, input_graph):\n    # Your calculation algorithm here\n    return np.delete(statistics, self._indices_to_ignore)\n</code></pre>"},{"location":"custom_metrics/#creating-exogenous-metrics","title":"Creating exogenous metrics","text":"<p>Metrics that use exogenous variables are based on the same principles as the previous example. The only difference is that the <code>__init__</code> method now receives a collection of exogenous variables that will be used in the <code>calculate</code> function. </p> <p>As an example, let's create a directed metric that receives an exogenous type for each node, and counts the number of connections between nodes of the same type - </p> <pre><code>class NumberOfConnectionsBetweenSameType(Metric):\n    def __init__(self, exogenous_types):\n        super().__init__(requires_graph=False)\n        self._is_directed = True\n        self._is_dyadic_independent = True\n        self._exogenous_types = exogenous_types\n\n    def calculate(self, input_graph: np.ndarray):\n        n = input_graph.shape[0]\n        num_connections = 0\n\n        for i in range(n):\n            for j in range(n):\n                if self._exogenous_types[i] == self._exogenous_types[j] and input_graph[i, j] == 1:\n                    num_connections += 1\n\n        return num_connections\n</code></pre> <p>Now let's verify the behavior of the metric - </p> <pre><code>W = np.array([\n    [0, 1, 0, 0],\n    [1, 0, 0, 1],\n    [1, 0, 0, 0],\n    [1, 0, 1, 0]\n])\n\ntypes = [1, 1, 2, 1]\n\nnum_connections_same_type = NumberOfConnectionsBetweenSameType(types)\nnum_connections = num_connections_same_type.calculate(W)\nprint(f\"# of connections between nodes of the same type: {num_connections}\")\n</code></pre> <p>Output -  <pre><code># of connections between nodes of the same type: 4\n</code></pre></p> <p>There are 3 nodes of the same type (nodes 1, 2, and 4 are all of type <code>1</code>), and there are 4 connections between them -  <pre><code>1 --&gt; 2 , 2 --&gt; 1, 2 --&gt; 4, 4 --&gt; 1\n</code></pre></p>"},{"location":"custom_metrics/#additional-settings-and-configurations","title":"Additional settings and configurations","text":""},{"location":"custom_metrics/#naming-a-metric","title":"Naming a metric","text":"<p>A metric should be named in a way that is descriptive of what it calculates. The metric name is determined by implementing a <code>__str()__</code> function in the metric class. For example, the <code>NumberOfBigIndegreeNodes</code> metric could be named as follows - </p> <pre><code>class NumberOfBigIndegreeNodes(Metric):\n    def __str__(self):\n        return \"num_big_indegree_nodes\"\n</code></pre>"},{"location":"custom_metrics/#speeding-up-metric-calculations","title":"Speeding up metric calculations","text":"<p>TODO</p>"},{"location":"custom_metrics/#testing-your-metric","title":"Testing your metric","text":"<p>It is recommended that you test the behavior of your metric to ensure that it behaves as expected. This can be done by writing simple test functions and running them on a variety of graphs. Some things to consider when testing your metric -</p> <ul> <li>Does the <code>calculate</code> method return the correct results?</li> <li>Does the metric correctly implement the <code>indices_to_ignore</code> attributes? Namely, does a metric with multiple statistics allow for the exclusion of certain statistics?</li> <li>If the metric is dyadic dependent, does it correctly implement the <code>dyadic_dependent</code> attribute, forcing the use of <code>MCMLE</code>?</li> </ul>"},{"location":"fitting/","title":"Fitting a model","text":"<p>Fitting an ERGM model requires a graph and a set of statistics that describe the graph. The model is then fit by maximizing the likelihood of the observed graph under the model. </p> <p>The following example demonstrates how to fit a simple ERGM model from Sampson's monastery data.</p> <pre><code>from pyERGM.ergm import ERGM\nfrom pyERGM.metrics import *\nfrom pyERGM.datasets import load_sampson\n\nsampson_matrix = load_sampson()\n\nnum_nodes = sampson_matrix.shape[0]\nis_directed = True\nmetrics = [NumberOfEdgesDirected(), TotalReciprocity()]\n\nmodel = ERGM(num_nodes, metrics, is_directed=is_directed)\nmodel.fit(sampson_matrix)\n</code></pre> <p>The above example fits a model from the Sampson's monastery data using the number of edges and total reciprocity as statistics. The graph is represented as an adjacency matrix, but pyERGM also supports graphs represented as <code>networkx</code> objects.</p>"},{"location":"fitting/#ergm","title":"ERGM","text":"<pre><code>class pyERGM.ERGM(n_nodes, metrics_collection, is_directed, **kwargs)\n</code></pre> <p>Parameters:</p> <ul> <li>n_nodes (int) - Number of nodes in the graph.</li> <li>metrics_collection (Collection[Metric]) - A list of Metric objects for calculating statistics of a graph.</li> <li>is_directed (bool) - Whether the graph is directed or not.</li> <li>initial_thetas (np.ndarray) - Optional. The initial values of the coefficients of the ERGM. If not provided, they are randomly initialized.</li> <li>initial_normalization_factor (float) - Optional. The initial value of the normalization factor. If not provided, it is randomly initialized.</li> <li>seed_MCMC_proba (float) - Optional. The probability of a connection in the seed graph for MCMC sampling, in case no seed graph is provided. Defaults to 0.25</li> <li>sample_size (int) - Optional. The number of graphs to sample via MCMC. If number of samples is odd, it is increased by 1. This is because downstream algorithms assume the sample size is even (e.g. the Covariance matrix estimation). Defaults to 1000</li> <li>use_sparse_matrix (bool) - Optional. Whether to use sparse matrices for the adjacency matrix.  Sparse matrices are implemented via PyTorch's Sparse Tensor's, which are still in beta.  Defaults to False</li> <li>fix_collinearity (bool) - Optional. Whether to fix collinearity in the metrics. Defaults to True</li> <li>collinearity_fixer_sample_size (int) - Optional. The number of graphs to sample for fixing collinearity. Defaults to 1000</li> </ul>"},{"location":"fitting/#fit","title":"fit","text":"<p><pre><code>pyERGM.ERGM.fit(observed_graph, **kwargs)\n</code></pre> Fit an ERGM model to a given graph with one of the two fitting methods - MPLE or MCMLE.</p> <p>With the exception of dyadic dependent models, all models are fit using the MCMLE method. Dyadic dependent models are fit using the MPLE method, which simply amounts to running a logistic regression.</p> <p>Parameters:</p> <ul> <li>observed_graph (np.ndarray) - The adjacency matrix of the observed graph. #TODO - how do we support nx.Graph?</li> <li>lr (float) - Optional. The learning rate for the optimization. Defaults to 0.1</li> <li>opt_steps (int) - Optional. The number of optimization steps to run. Defaults to 1000</li> <li>steps_for_decay (int) - Optional. The number of steps after which to decay the optimization params. Defaults to 100 # TODO - redundant parameter?</li> <li>lr_decay_pct (float) - Optional. The decay factor for the learning rate. Defaults to 0.01</li> <li>l2_grad_thresh (float) - Optional. The threshold for the L2 norm of the gradient to stop the optimization. Relevant only for convergence criterion <code>zero_grad_norm</code>. Defaults to 0.001</li> <li>sliding_grad_window_k (int) - Optional. The size of the sliding window for the gradient, for which we use to calculate the mean gradient norm. This value is then tested against l2_grad_thresh to decide whether optimization halts.Relevant only for convergence criterion <code>zero_grad_norm</code>. Defaults to 10</li> <li>max_sliding_window_size (int) - Optional. The maximum size of the sliding window for the gradient. Relevant only for convergence criterion <code>zero_grad_norm</code>. Defaults to 100</li> <li>max_nets_for_sample (int) - Optional. The maximum number of graphs to sample with MCMC. Defaults to 1000 #TODO - Do we still need this? Seems like increasing the sample size isn't necessary (we'll gonna pick large sample sizes anyway)    </li> <li>sample_pct_growth (float) - Optional. The percentage growth of the number of graphs to sample, which we want to increase over time. Defaults to 0.02. #TODO - Same as <code>max_nets_for_sample</code>. Do we still need this?</li> <li>optimization_method (str) - Optional. The optimization method to use. Can be either \"newton_raphson\" or \"gradient_descent\". Defaults to \"newton_raphson\".</li> <li>convergence_criterion (str) - Optional. The criterion for convergence. Can be either \"hotelling\" or \"zero_grad_norm\". Defaults to \"zero_grad_norm\". # TODO - Revisit this when we fix convergence criterion.</li> <li>cov_matrix_estimation_method (str) - Optional. The method to estimate the covariance matrix. Supported methods - <code>naive</code>, <code>batch</code>, <code>multivariate_initial_sequence</code>. Defaults to \"batch\".</li> <li>cov_matrix_num_batches (int) - Optional. The number of batches to use for estimating the covariance matrix.Relevant only for <code>cov_matrix_estimation_method=\"batch\"</code>. Defaults to 25.</li> <li>hotelling_confidence (float) - Optional. The confidence level for the Hotelling's T-squared test. Defaults to 0.99.</li> <li>theta_init_method (str) - Optional. The method to initialize the theta values. Can be either \"uniform\" or \"mple\". The MPLE method can be used even for dyadic dependent models, since it serves as a good starting point for the MCMLE. Defaults to \"mple\".</li> <li>no_mple (bool) - Optional. Whether to skip the MPLE step and go directly to MCMLE. Defaults to False.</li> <li>mcmc_burn_in (int) - Optional. The number of burn-in steps for the MCMC sampler. Defaults to 1000.</li> <li>mcmc_steps_per_sample (int) - Optional. The number of steps to run the MCMC sampler for each sample. Defaults to 10.</li> <li>mcmc_seed_network (np.ndarray) - Optional. The seed network for the MCMC sampler. If not provided, the thetas that are currently set are used to calculate the MPLE prediction, from which the sample is drawn. Defaults to None.</li> <li>mcmc_sample_size (int) - Optional. The number of networks to sample with the MCMC sampler. Defaults to 100.</li> <li>mple_lr (float) - Optional. The learning rate for the logistic regression model in the MPLE step. Defaults to 0.001.</li> <li>mple_stopping_thr (float) - Optional. The stopping threshold for the logistic regression model in the MPLE step. Defaults to 1e-6.</li> <li>mple_max_iter (int) - Optional. The maximum number of iterations for the logistic regression model in the MPLE step. Defaults to 1000.</li> </ul> <p>Returns:</p> <ul> <li>grads (np.ndarray) - The gradients of the model parameters.</li> <li>hotelling_statistics (list) - The Hotelling's T-squared statistics for the model parameters. </li> </ul>"},{"location":"fitting/#print_model_parameters","title":"print_model_parameters","text":"<p><pre><code>pyERGM.ERGM.print_model_parameters()\n</code></pre> Prints the parameters of the ERGM model.</p>"},{"location":"metrics/","title":"Introduction to graph metrics","text":"<p>Calculating a statistics vectors for a graph, \\(g(y)\\), is a crucial component of ERGMs.  Formally, \\(g\\) is a function that receives a graph and returns a vector \\(g(y) \\in \\mathbb{R}^q\\) with some statistics on that graph. These statistics can be as simple as counting the number of edges in the graph, calculating the number of triangles, or even more complex statistics that depend on exogenous variables for each node.</p> <p>pyERGM provides a collection of metrics that can be used to calculate these statistics. These metrics are implemented as classes that inherit from the <code>Metric</code> class.</p>"},{"location":"metrics/#the-metric-class","title":"The <code>Metric</code> class","text":"<p>Every metric in pyERGM is inherited from the <code>Metric</code> class, which defines the interface for calculating statistics on a graph. All metrics implement a <code>calculate</code> function, which receives a graph and returns the calculated statistics.</p> <p><pre><code>pyERGM.metrics.Metric.calculate(input: np.ndarray | nx.Graph)\n</code></pre> Parameters:</p> <ul> <li>input (np.ndarray | nx.Graph) - The input graph for which to calculate the statistics. The graph can be represented as a numpy adjacency matrix or a <code>networkx</code> graph object.</li> </ul> <p>Returns:</p> <ul> <li>result (np.ndarray) - The calculated statistics vector of length \\(q \\geq 1\\), depending on how many  statistics the metric returns.</li> </ul>"},{"location":"metrics/#examples","title":"Examples","text":""},{"location":"metrics/#basic-metrics","title":"Basic metrics","text":"<p>As our first example, we will calculate two statistics for a directed graph with 4 nodes -</p> <ul> <li>The number of edges in the graph</li> <li>The number of reciprocal edges in the graph (i.e. how many node pairs have reciprocal edges between them).</li> </ul> <pre><code>import numpy as np\nfrom pyERGM.metrics import NumberOfEdgesDirected, TotalReciprocity\n\n# Create a connectivity matrix for a directed graph with 4 nodes\nW = np.array([\n    [0, 1, 0, 0],\n    [1, 0, 0, 1],\n    [1, 1, 0, 0],\n    [0, 0, 1, 0]\n])\n\n\nnum_edges = NumberOfEdgesDirected()\ntotal_reciprocity = TotalReciprocity()\n\nprint(f\"Number of edges: {num_edges.calculate(W)}\")\nprint(f\"Total reciprocity: {total_reciprocity.calculate(W)}\")\n</code></pre> <p>Output: <pre><code>Number of edges: 6\nTotal reciprocity: 1\n</code></pre></p> <p>As expected, the graph has 6 edges and 1 reciprocal edge between nodes 1 and 2.</p>"},{"location":"metrics/#metrics-with-multiple-statistics","title":"Metrics with multiple statistics","text":"<p>As opposed to the <code>NumberOfEdgesDirected</code> metric which returns a scalar, some metrics return a vector of statistics. For example, the <code>InDegree</code> and <code>OutDegree</code> metrics calculate the indegrees and outdegrees of each node in the graph.</p> <pre><code>from pyERGM.metrics import InDegree, OutDegree\n\nindeg = InDegree()\noutdeg = OutDegree()\n\nprint(f\"In-degree: {indeg.calculate(W)}\")\nprint(f\"Out-degree: {outdeg.calculate(W)}\")\n</code></pre> <p>Output: <pre><code>In-degree: [2 2 1 1]\nOut-degree: [1 2 2 1]\n</code></pre></p>"},{"location":"metrics/#exogenous-metrics","title":"Exogenous metrics","text":"<p>So far we've seen metrics that are only based on the graph's connectivity matrix. However, there are many scenarios in which the graph nodes &amp; edges have additional attributes that are external to the connectivity matrix. For example, in a graph that represents a social network, each node might have an attribute representing the age of a person. These are called exogenous attributes. </p> <p>The exogenous attributes are passed to the metric as a collection of external attributes. The order of these attributes should correspond to the node order in the connectivity matrix.</p> <p>In the following example, we take a graph with 3 nodes, and assign a number to each node. We then wish to sum these attributes across nodes that are connected to each other. This is done using the <code>NodeAttrSum</code> metric.</p> <pre><code>from pyERGM.metrics import NodeAttrSum\n\nW = np.array([\n    [0, 1, 0],\n    [1, 0, 0],\n    [1, 0, 0],\n])\n\n# For each of the 3 nodes in W, we assign a number.\n# The order of these attributes corresponds to the node order in the connectivity matrix.\nexternal_attributes = [2, 1, 5]\n\nnode_attr_metric = NodeAttrSum(external_attributes, is_directed=True)\n\nprint(f\"Sum of attributes: {node_attr_metric.calculate(W)}\")\n</code></pre> <p>Output: <pre><code>Sum of attributes: [13]\n</code></pre></p> <p>Let's verify this calculation, by observing the attribute sum for every edge - </p> <ul> <li>Edge 1 \\(\\rightarrow\\) 2 : 2 + 1 = 3</li> <li>Edge 2 \\(\\rightarrow\\) 1 : 1 + 2 = 3</li> <li>Edge 3 \\(\\rightarrow\\) 1 : 5 + 2 = 7</li> </ul> <p>which sums up to 13.</p>"},{"location":"metrics/#a-collection-of-metrics","title":"A collection of metrics","text":"<p>It is often the case that an ERGM uses more than a single metric. To facilitate this, pyERGM provides a <code>MetricsCollection</code> class that can be used to collect multiple metrics together.</p> <p>When an ERGM is initialized, it receives a list of metrics. For example - </p> <pre><code>from pyERGM.metrics import NumberOfEdgesUndirected, NumberOfTriangles\nmetrics = [NumberOfEdgesUndirected(), NumberOfTriangles()]\nmodel = ERGM(num_nodes, metrics, is_directed=False)\n</code></pre> <p>The <code>MetricsCollection</code> object is initialized behind the scenes by the ERGM constructor, and is used throughout the model's lifecycle to calculate the statistics vector for the graph.</p>"},{"location":"metrics/#collinearity-between-metrics","title":"Collinearity between metrics","text":"<p>Collineraity is a common issue in regression models, where two or more variables have some form of linear relationship. The degree to which the variables are correlated varies from perfect collinearity, where a pair of variables is perfectly correlated, to weaker forms of collinearity, in which the variables are correlated but not perfectly so. Collinearity can lead to issues in the optimization process, as well as in the interpretation of the model's coefficients.</p> <p>As an example, let's initialize a model with two metrics - one that counts the number of edges, and one that counts the degree of every node in the graph.</p> <ol> <li><code>NumberOfEdgesUndirected</code></li> <li><code>UndirectedDegree</code></li> </ol> <pre><code>from pyERGM.metrics import NumberOfEdgesUndirected, UndirectedDegree\n\nW = np.array([\n    [0, 1, 0],\n    [1, 0, 0],\n    [1, 0, 0],\n])\n\n\nnum_edges = NumberOfEdgesUndirected()\ndegrees = UndirectedDegree()\n\nprint(f\"Number of edges: {num_edges.calculate(W)}\")\nprint(f\"Degrees: {degrees.calculate(W)}\")\n</code></pre> <p>Output: <pre><code>Number of edges: 3\nDegrees: [2, 1, 0]\n</code></pre></p> <p>In this example, there exists perfect collinearity between the number of edges and degree profile of all nodes in the graph. By definition, in an undirected graph the sum of degrees equals the number of edges, deeming one of these regressors to be redundant. This means that any regression model that includes all the 4 regressons will have perfect collinearity, which can lead to issues in the optimization process. </p> <p>The most straightforward solution is to remove one of the conflicting metrics. pyERGM offers two ways to handle such issues - </p> <p>Automatic removal - The <code>MetricsCollection</code> class provides a tool for automatically detecting and removing collinear metrics. This is the default setting, and can be turned off with the <code>fix_collinearity=False</code> flag in the ERGM constructor. (TODO - elaborate on the algorithm).</p> <p>Manual removal - As the user building the model, you can manually remove metrics that you know are collinear. This can either be done in any of the following ways - </p> <ol> <li>Removing a full metric - Removing the metric from the list of metrics passed to the ERGM constructor is the most straightforward way to fix collinearity, but it might not always be the best solution. In the example above, the <code>NumberOfEdgesUndirected</code> metric seems more informative than the <code>UndirectedDegree</code> metric, so completely removing it might not be desired.</li> <li>Ignoring a specific metric statistic - Metrics with more than a single statistic have an <code>indices_to_ignore</code> attribute that can be used to ignore specific statistics within the metric. It is a list of indices that should be ignored when calculating the statistics vector, and is passed when initializing the metric.  The order of indices within the metric depends on the metric type - <ul> <li>If the metric statistics depend only on the graph's connectivity matrix (e.g. <code>UndirectedDegree</code>), the indices correspond to the original ordering of the nodes in the graph. </li> <li>If the metric statistics depend on exogenous attributes, the indices correspond to the lexicographic ordering of the exogenous attributes passed to the metric, i.e. <code>sorted(external_attributes)</code>. In a metric such as <code>NumberOfEdgesTypesDirected</code> that creates statistics for type pairs, the ordering is the lexicographic ordering of the cartesian product of the node types. For example if <code>external_attributes=[\"A\", \"A\", \"B\"]</code>, the metric will produce features for type pairs <code>[\"AA\", \"AB\", \"BA\", \"BB\"]</code>, in a lexicographical ordering.</li> </ul> </li> </ol> <p>Let's see how we could have fixed the collinearity issue in the previous example.</p> <p>Automatic fixing -  <pre><code>from pyERGM.metrics import NumberOfEdgesUndirected, UndirectedDegree\nfrom pyERGM.metrics import MetricsCollection\n\nW = np.array([\n    [0, 1, 0],\n    [1, 0, 0],\n    [1, 0, 0],\n])\n\nnum_edges = NumberOfEdgesUndirected()\ndegrees = UndirectedDegree()\n\nmetrics = MetricsCollection(metrics=[num_edges, degrees], is_directed=False, n_nodes=3, fix_collinearity=True)\n</code></pre></p> <p>which will output -  <pre><code>Removing the 0 feature of undirected_degree to fix multi-collinearity\n</code></pre> telling us that the 0-th idx feature of the <code>UndirectedDegree</code> metric was removed to fix the collinearity issue. We can also verify this by running - </p> <pre><code>print(f\"Ignored features - \")\nprint(metrics.get_ignored_features())\n\nprint(f\"Remaining features\")\nprint(metrics.get_parameter_names())\n</code></pre> <p>with outputs - <pre><code>Ignored features -\n('undirected_degree_1',)\nRemaining features\n('num_edges_undirected', 'undirected_degree_2', 'undirected_degree_3')\n</code></pre></p> <p>Ignoring the first statistic in <code>UndirectedDegree</code> - <pre><code>from pyERGM.metrics import NumberOfEdgesUndirected, UndirectedDegree\n\nW = np.array([\n    [0, 1, 0],\n    [1, 0, 0],\n    [1, 0, 0],\n])\n\nnum_edges = NumberOfEdgesUndirected()\ndegrees = UndirectedDegree(indices_to_ignore=[0])\n\nprint(f\"Number of edges: {num_edges.calculate(W)}\")\nprint(f\"Degrees: {degrees.calculate(W)}\")\n</code></pre></p> <p>Output: <pre><code>Number of edges: 3\nDegrees: [1, 0]\n</code></pre></p>"},{"location":"overview/","title":"Overview","text":""},{"location":"overview/#what-is-an-ergm","title":"What is an ERGM ?","text":"<p>Exponential Random Graph Models (ERGMs) are a class of statistical models that describe the distribution of random graphs. An ERGM defines a random variable \\(\\mathbf{Y}\\), which is simply a random graph on \\(n\\) nodes. The probability of observing a graph \\(y\\in\\mathcal{Y}\\) is given by - </p> \\[\\Pr(\\mathbf{Y}=y | \\theta) = \\frac{\\exp(\\theta^Tg(y))}{\\sum_{z\\in\\mathcal{Y}} \\exp(\\theta^Tg(z))}\\] <p>where \\(\\mathcal{Y}\\) is the set of all \\(n\\) node graphs, \\(g(y)\\) is a vector of statistics that describe the graph \\(y\\), and \\(\\theta \\in \\mathbb{R}^q\\) is a vector of model parameters. Each graph is represented by a binary adjacency matrix, where \\(y_{ij}=1\\) if there is an edge between nodes \\(i\\) and \\(j\\) (and \\(y_{ji}=1\\) in the undirected case).</p> <p>An important property of ERGMs is that they are subject to the maximum entropy principle - the optimal model is the one that maximizes the entropy subject to the constraints imposed by the graph statistics \\(g(\\cdot)\\). This makes ERGMs a powerful tool for modeling complex graph structures, since they ...? #TODO</p>"},{"location":"overview/#how-to-fit-an-ergm","title":"How to fit an ERGM?","text":"<p>Given a graph \\(y_{\\text{obs}}\\) and a set of statistics \\(g(y)\\), the model is fit by maximizing the likelihood of the observed graph under the model. The log-likelihood function is defined as follows - </p> \\[\\ell(\\theta | y_{\\text{obs}}) = \\theta^Tg(y_{\\text{obs}}) - \\log(\\sum_{z\\in\\mathcal{Y}} \\exp(\\theta^Tg(z)))\\] <p>which is optimized using numerical optimization techniques (such as Gradient Descent, Newton-Raphson, etc.) to find the optimal parameter vector - </p> \\[\\theta^* = \\arg \\max \\ \\ell(\\theta | y_{\\text{obs}}) = \\arg \\min \\ - \\ell(\\theta | y_{\\text{obs}})\\] <p>Generally speaking, there are two ways for fitting an ERGM - MCMLE and MPLE.</p>"},{"location":"overview/#mcmle","title":"MCMLE","text":"<p>Monte Carlo maximum likelihood estimation (MCMLE) is the main method for fitting ERGMs, and can be used it any setting, no matter what statistics \\(g(y)\\) are calculated on a graph.</p> <p>The algorithm can be sketched as follows - </p> <ol> <li>Pick a random starting point \\(\\theta_0\\).</li> <li>Calculate the gradient of the log-likelihood function \\(\\nabla \\ell(\\theta | y_{\\text{obs}}) = g(y_{\\text{obs}})- \\mathbb{E}_{z\\sim\\mathcal{Y}}[g(z)]\\) (see Appendix for the full derivation).  Calculating the expectation \\(\\mathbb{E}_{z\\sim\\mathcal{Y}}[g(z)]\\) is computationally infeasible, so we approximate it by sampling graphs from the distribution. Graphs can be sampled using an MCMC algorithm, such as the Metropolis-Hastings algorithm.</li> <li>When using the Newton-Raphson method, calculate the Hessian matrix of the log-likelihood function -  \\(H_{i, j} = \\mathbb{E}_{z}[g_i(z)]\\mathbb{E}_{z}[g_j(z)] - \\mathbb{E}_{z}[g_i(z)g_j(z)]\\)</li> <li>In every iteration, update the parameter vector \\(\\theta\\) using the gradient and Hessian - \\(\\theta_{\\text{new}} = \\theta_{\\text{old}} - \\eta \\cdot H^{-1}\\nabla \\ell(\\theta | y_{\\text{obs}})\\) where \\(\\eta\\) is the learning rate. (When using the Gradient Descent method, the Hessian is not used.)</li> <li>Repeat until optimization converges or reaches a maximum number of iterations.</li> </ol>"},{"location":"overview/#convergence-criterions","title":"Convergence criterions","text":"<p>pyERGM supports two convergence criterions for the optimization -</p> <ul> <li>zero_grad_norm - The optimization stops when the L2 norm of the gradient falls below a threshold.</li> <li>hotelling - The optimization stops when the Hotelling's \\(T^2\\) statistic falls below a threshold.</li> </ul> <p>TODO - Elaborate on the convergence criterions.</p>"},{"location":"overview/#mple","title":"MPLE","text":""},{"location":"overview/#how-to-sample-from-an-ergm","title":"How to sample from an ERGM?","text":"<p>It is computationally infeasible to directly draw graphs from the distribution - Calculating the normalization factor \\(Z=\\sum_{z\\in\\mathcal{Y}} \\exp(\\theta^Tg(z))\\) requires an iteration over the entire space of graphs, which is exponential in size. Instead, we can use Markov Chain Monte Carlo (MCMC) methods to sample from the distribution. The Metropolis-Hastings algorithm is a popular choice for sampling from ERGMs, and is the default sampling method in pyERGM.</p> <p>In it's simplest form the Metropolis-Hastings algorithm iteratively collects samples from the distribution by following these steps -</p> <ol> <li>Start with a randomly picked initial graph \\(y_0 \\in \\mathcal{Y}\\), and set as the current graph \\(y_\\text{current}=y_0\\).</li> <li>Propose a new graph \\(y_\\text{proposed}\\) by making a small change to \\(y_\\text{current}\\)  either by adding or removing an edge between randomly picked nodes \\(i, j\\).</li> <li>Calculate the change score, defined as \\(\\delta_g(y)_{i,j} = g(y_{\\text{proposed}}) - g(y_{\\text{current}})\\)</li> <li>Accept the proposed graph with probability \\(p_{\\text{accept}}=\\min\\left(1, \\exp(\\theta^T \\delta_g(y)_{i,j}) \\right)\\).</li> </ol> <p>Steps 2-4 are repeated for a large number of iterations, until a sufficient number of graphs are collected.</p>"},{"location":"overview/#further-reading","title":"Further Reading","text":"<p>pyERGM is inspired by the <code>ergm</code> package in <code>R</code>. For a good introduction to ERGMs, we recommend the following resources:</p> <ul> <li>ergm: A Package to Fit, Simulate and Diagnose Exponential-Family Models for Networks, Hunter et al. (2008)</li> <li>ergm 4: Computational Improvements, Krivitsky et al. (2022)</li> <li>ERGM introduction for Social Network Analysis is a workshop on Social Network Analysis from Stanford University, which provides a good example for using ERGMs in practice, using the <code>R</code> package.</li> </ul>"},{"location":"overview/#appendix","title":"Appendix","text":""},{"location":"overview/#deriving-the-gradient-and-hessian","title":"Deriving the gradient and Hessian","text":"<p>Given a graph \\(y_{\\text{obs}}\\), we can treat the probability function as a likelihood function \\(\\ell(\\theta | y_{\\text{obs}})\\), which defines the log likelihood function \\(\\ell(\\theta)\\) - </p> \\[\\ell(\\theta | y_{\\text{obs}}) = \\theta^Tg(y_{\\text{obs}}) - \\log(\\sum_{z\\in\\mathcal{Y}} \\exp(\\theta^Tg(z)))\\] <p>which can be optimized to obtain - </p> \\[ \\theta^* = \\arg \\max \\ \\ell(\\theta | y_{\\text{obs}}) = \\arg \\min \\ - \\ell(\\theta | y_{\\text{obs}}) \\] <p>The log-likelihood funcrtion can be differentiated with respect to \\(\\theta\\) to obtain the gradient -</p> \\[ \\frac{\\partial}{\\partial \\theta} \\ \\ell (\\theta) = g(y_{\\text{obs}}) - \\frac{\\sum_{z\\in\\mathcal{Y}} \\exp(\\theta^Tg(z))g(z)}{\\sum_{z\\in\\mathcal{Y}} \\exp(\\theta^Tg(z))}  \\] \\[ = g(y_{\\text{obs}}) - \\sum_{z\\in\\mathcal{Y}} \\frac{\\exp(\\theta^Tg(z))}{Z}g(z) \\] \\[ = g(y_{\\text{obs}})- \\sum_{z\\in\\mathcal{Y}}\\Pr_{\\theta, \\mathcal{Y}}(\\mathbf{Y}=z)g(z)  \\] \\[ = g(y_{\\text{obs}})- \\mathbb{E}_{z\\sim\\mathcal{Y}}[g(z)]  \\] <p>where \\(Z=\\sum_{z\\in\\mathcal{Y}} \\exp(\\theta^Tg(z))\\) is the normalization factor.</p> <p>We can now take the second derivative, to find the \\(i, j\\)-th entry of the Hessian matrix -</p> \\[ \\frac{\\partial^2\\ell(\\theta)}{\\partial \\theta_i \\theta_j} = \\frac{\\partial}{\\partial \\theta_j} \\big(g_i(y_{\\text{obs}}) - \\mathbb{E}_{z\\sim\\mathcal{Y}}[g_i(z)]  \\big)  = -\\sum_{z\\in \\mathcal{Y}} \\frac{\\partial}{\\partial \\theta_j} \\big( \\frac{\\exp(\\theta^T g(z)) \\cdot g_i(z)}{Z} \\big) \\] <p>The derivative of the summed term can be calculated as follows - </p> \\[ \\frac{\\partial}{\\partial \\theta_j} \\frac{\\exp(\\theta^T g(z)) }{Z}\\cdot g_i(z) = \\frac{\\exp(\\theta^T g(z))g_j(z)Z - \\exp(\\theta^T g(z)) \\cdot \\frac{\\partial}{\\partial \\theta_j}Z }{Z^2} \\cdot g_i(z) \\] \\[ = \\frac{\\exp(\\theta^T g(z))}{Z^2} \\cdot g_i(z) \\cdot \\big( g_j(z)Z - \\sum_{z\\in \\mathcal{Y}} \\exp(\\theta^T g(z)) g_j(z)  \\big) \\] \\[ = \\frac{\\exp(\\theta^T g(z))}{Z} \\cdot g_i(z)\\cdot (g_j(z) - \\sum_{z\\in \\mathcal{Y}}\\Pr[Y=z]\\cdot g_j(z)) \\] \\[ = \\frac{\\exp(\\theta^T g(z))}{Z} \\cdot g_i(z)\\cdot (g_j(z) - \\mathbb{E}_{z\\sim\\mathcal{Y}}[g_j(z)]) \\] <p>which can now be plugged back - </p> \\[ \\frac{\\partial^2\\ell(\\theta)}{\\partial \\theta_i \\theta_j} = -\\sum_{z\\in\\mathcal{Y}} \\frac{\\exp(\\theta^T g(z))}{Z} \\cdot g_i(z)\\cdot (g_j(z) - \\mathbb{E}_{z\\sim\\mathcal{Y}}[g_j(z)]) \\] \\[ = -\\sum_{z\\in\\mathcal{Y}}\\Pr[Y=z]g_i(z)g_j(z) + \\sum_{z\\in\\mathcal{Y}}\\Pr[Y=z]g_i(z)\\mathbb{E}_{z\\sim\\mathcal{Y}}[g_j(z)] \\] \\[ = \\mathbb{E}_{z}[g_i(z)]\\mathbb{E}_{z}[g_j(z)] - \\mathbb{E}_{z}[g_i(z)g_j(z)] \\] <p>This derivative defines the \\(i, j\\)-th entry of the Hessian matrix.</p>"}]}